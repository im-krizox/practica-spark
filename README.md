# Práctica Spark: Programación Paralela y Concurrente

Este proyecto forma parte del curso de **Programación Paralela y Concurrente** y tiene como objetivo explorar y aplicar conceptos fundamentales de procesamiento distribuido utilizando **Apache Spark**. A través de esta práctica, se busca desarrollar habilidades en el manejo de grandes volúmenes de datos y la implementación de algoritmos paralelos eficientes.

## Objetivos del Proyecto

- Comprender los principios básicos de Apache Spark y su ecosistema.
- Implementar soluciones paralelas para problemas comunes de procesamiento de datos.
- Analizar el rendimiento y la escalabilidad de las aplicaciones distribuidas.
- Aplicar técnicas de optimización para mejorar la eficiencia de los trabajos en Spark.

## Tecnologías Utilizadas

- **Apache Spark**: Framework para procesamiento distribuido.
- **Python**: Lenguaje de programación para la implementación de los algoritmos.
- **Jupyter Notebooks**: Para experimentación y visualización de resultados.

## Estructura del Proyecto

- `src/`: Código fuente de las implementaciones.
- `data/`: Conjunto de datos utilizados para las pruebas.
- `docs/`: Documentación adicional del proyecto.
- `results/`: Resultados obtenidos de las ejecuciones.

## Requisitos Previos

- Conocimientos básicos de programación paralela y concurrente.
- Familiaridad con Apache Spark y su API.
- Entorno configurado con Spark y dependencias necesarias.

## Ejecución del Proyecto

1. Clonar el repositorio:
    ```bash
    git clone <git@github.com:im-krizox/practica-spark.git>
    cd practica-spark
    ```
2. Configurar el entorno de ejecución (Spark, dependencias, etc.).
3. Ejecutar los scripts o notebooks según las instrucciones en `docs/`.

## Licencia

Este proyecto se distribuye bajo la licencia MIT. Consulta el archivo `LICENSE` para más detalles.